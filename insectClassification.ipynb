{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "\n",
    "from tensorflow.keras.utils import  img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense \n",
    "from keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import math \n",
    "import itertools, re, os, random, keras\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path ='insec/train/'\n",
    "valid_path ='insec/valid/'\n",
    "test_path ='insec/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotImagesAndLabels(folder):\n",
    "    c=1\n",
    "    directory=os.listdir(folder)\n",
    "    plt.figure(figsize=(28,20))\n",
    "    for each in directory:\n",
    "        currentFolder=folder+ \"\\\\\" +each\n",
    "        for i, file in enumerate(os.listdir(currentFolder)[0:4]):\n",
    "            full_path=currentFolder+\"\\\\\"+file\n",
    "            plt.subplot(3, 8, c)\n",
    "            img = mpimg.imread(full_path)\n",
    "            plt.imshow(img)\n",
    "            plt.title(each)\n",
    "            c+=1\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lets see some samples from training data...\")\n",
    "plotImagesAndLabels(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = 'bottleneck_fc_model_ResNet50V2.h5'\n",
    "\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 15 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "\n",
    "\n",
    "ResNet50 = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet')\n",
    "for layer in ResNet50.layers:\n",
    "        layer.trainable=False\n",
    "datagen = ImageDataGenerator(rescale=1. / 255) \n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    train_path, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = ResNet50.predict_generator(generator, predict_size_train) \n",
    " \n",
    "np.save('bottleneck_features_train_ResNet50V2.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "   valid_path, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_valid = ResNet50.predict_generator(generator, predict_size_train) \n",
    " \n",
    "np.save('bottleneck_features_valid_ResNet50.npy', bottleneck_features_valid)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "   test_path, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "\n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_test = ResNet50.predict_generator(generator, predict_size_train) \n",
    " \n",
    "np.save('bottleneck_features_test_ResNet50.npy', bottleneck_features_test)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_path, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load('bottleneck_features_train_ResNet50V2.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors P\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_top = datagen.flow_from_directory( \n",
    "   valid_path, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "valid_data = np.load('bottleneck_features_valid_ResNet50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "valid_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "valid_labels = to_categorical(valid_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_top = datagen.flow_from_directory( \n",
    "   test_path, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "test_data = np.load('bottleneck_features_test_ResNet50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "test_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#This is the best model we found.\n",
    "start = datetime.datetime.now()\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(224,\n",
    "                                  224,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "    layers.RandomBrightness(0.2),\n",
    "    layers.RandomTranslation(0.2, 0.2),\n",
    "  ]\n",
    ")\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "model.add(Dense(200, activation=keras.layers.LeakyReLU(alpha=0.3)))\n",
    "model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "model.add(Dropout(0.75)) \n",
    "model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3)))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "   optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "   metrics=['acc'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('resnet.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True, patience=10)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data, train_labels, batch_size=batch_size),\n",
    "   epochs=10,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(valid_data, valid_labels),\n",
    "   callbacks=[es,mc])\n",
    "model.save_weights(top_model_weights_path)\n",
    "\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    valid_data, valid_labels, batch_size=batch_size, verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss)) \n",
    "\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=np.round(model.predict(test_data),0)\n",
    "print('rounded test_labels',preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds=['Bees', 'Beetles', 'Buttefly', 'Cicada','Dragonfly','Grasshopper','Ladybird']\n",
    "classification_metrics=metrics.classification_report(test_labels,preds,target_names=birds)\n",
    "print(classification_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "categorical_test_labels = pd.DataFrame(test_labels).idxmax(axis=1)\n",
    "categorical_preds = pd.DataFrame(preds).idxmax(axis=1)\n",
    "conf_mat = confusion_matrix(categorical_test_labels, categorical_preds)\n",
    "\n",
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "   normalize=False,\n",
    "   title='Confusion matrix',\n",
    "   cmap=plt.cm.Blues):\n",
    " \n",
    "#Add Normalization Option\n",
    " \n",
    "   if normalize:\n",
    "     cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "     print(\"Normalized confusion matrix\")\n",
    "   else:\n",
    "     print('Confusion matrix, without normalization')\n",
    " \n",
    "# print(cm)\n",
    " \n",
    "   plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "   plt.title(title)\n",
    "   plt.colorbar()\n",
    "   tick_marks = np.arange(len(classes))\n",
    "   plt.xticks(tick_marks, classes, rotation=45)\n",
    "   plt.yticks(tick_marks, classes)\n",
    " \n",
    "   fmt = '.2f' if normalize else 'd'\n",
    "   thresh = cm.max() / 2.\n",
    "   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "      plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "   plt.tight_layout()\n",
    "   plt.ylabel('True label')\n",
    "   plt.xlabel('Predicted label')\n",
    "\n",
    "plot_confusion_matrix(conf_mat, classes = ['Bees', 'Beetles', 'Buttefly', 'Cicada', 'Dragonfly', 'Grasshopper', 'Ladybird'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
